{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1136,"sourceType":"modelInstanceVersion","modelInstanceId":982}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport tf_keras as keras\n\nfrom keras import utils\nfrom tensorflow_examples.models.pix2pix import pix2pix\n\ndataset = tfds.load('oxford_iiit_pet')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-13T19:11:47.934106Z","iopub.execute_input":"2024-05-13T19:11:47.934953Z","iopub.status.idle":"2024-05-13T19:11:48.044899Z","shell.execute_reply.started":"2024-05-13T19:11:47.934921Z","shell.execute_reply":"2024-05-13T19:11:48.044150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q git+https://github.com/tensorflow/examples.git","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:50:43.979379Z","iopub.execute_input":"2024-05-13T18:50:43.979741Z","iopub.status.idle":"2024-05-13T18:51:01.211011Z","shell.execute_reply.started":"2024-05-13T18:50:43.979712Z","shell.execute_reply":"2024-05-13T18:51:01.209741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = (128, 128)\nEPOCHS = 50\n\nTRAIN_LENGTH = len(dataset['train'])\nBATCH_SIZE = 64\nBUFFER_SIZE = 1000\nSTEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:55:08.671268Z","iopub.execute_input":"2024-05-13T18:55:08.672163Z","iopub.status.idle":"2024-05-13T18:55:08.677389Z","shell.execute_reply.started":"2024-05-13T18:55:08.672129Z","shell.execute_reply":"2024-05-13T18:55:08.676436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"При анализе данных было выявлено, что размер изображений разный. \nМногие модели требуют одинаковый размер входных данных, чтобы модель могла правильно их обрабатывать.","metadata":{}},{"cell_type":"markdown","source":"# Разделение датасета на тренировочный и тестовый. Предобработка данных.","metadata":{}},{"cell_type":"markdown","source":"1. Нормализация. Приводим значения пикселей изображений с диапазона [0; 255] к диапазону [0; 1]\n2. Приводим изображения и маски к одному размеру.\n\nНормализация данных гаранитрует, что модель получает однотипные входные данные, что упрощает обучение и повышает ее точность.\nПриведение данных в разных единицах измерения и диапазонах значений к единому виду позволяет сравнивать их между собой или использовать для расчёта схожести объектов. \nМодель эффективнее извлекает признаки и быстрее обучается.","metadata":{}},{"cell_type":"code","source":"def resize_normalize(input_image, input_mask):\n    input_image = tf.image.resize(input_image, IMG_SIZE)\n    input_mask = tf.image.resize(input_mask, IMG_SIZE)\n    input_image = tf.cast(input_image, tf.float32) / 255.0\n    input_mask -= 1\n    return input_image, input_mask\n\ndef load_image_train(example):\n    return resize_normalize(example['image'], example['segmentation_mask'])\n\ndef load_image_test(example):\n    return resize_normalize(example['image'], example['segmentation_mask'])","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:51:57.913582Z","iopub.execute_input":"2024-05-13T18:51:57.914224Z","iopub.status.idle":"2024-05-13T18:51:57.920417Z","shell.execute_reply.started":"2024-05-13T18:51:57.914172Z","shell.execute_reply":"2024-05-13T18:51:57.919404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.AUTOTUNE)\ntest_dataset = dataset['test'].map(load_image_test)\n\ntrain_data = train_dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\ntrain_data = train_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\ntest_data = test_dataset.batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:52:00.341637Z","iopub.execute_input":"2024-05-13T18:52:00.342499Z","iopub.status.idle":"2024-05-13T18:52:00.534317Z","shell.execute_reply.started":"2024-05-13T18:52:00.342466Z","shell.execute_reply":"2024-05-13T18:52:00.533358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display(display_list):\n  plt.figure(figsize=(15,15))\n\n  title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n  for i in range(len(display_list)):\n    plt.subplot(1, len(display_list), i+1)\n    plt.title(title[i])\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n    plt.axis('off')\n  plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:52:02.899967Z","iopub.execute_input":"2024-05-13T18:52:02.900356Z","iopub.status.idle":"2024-05-13T18:52:02.906883Z","shell.execute_reply.started":"2024-05-13T18:52:02.900327Z","shell.execute_reply":"2024-05-13T18:52:02.905837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for image, mask in train_dataset.take(10):\n  sample_image, sample_mask = image, mask\ndisplay([sample_image, sample_mask])\nfor image, mask in train_dataset.take(20):\n  sample_image, sample_mask = image, mask\ndisplay([sample_image, sample_mask])","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:52:06.027883Z","iopub.execute_input":"2024-05-13T18:52:06.028842Z","iopub.status.idle":"2024-05-13T18:52:07.224463Z","shell.execute_reply.started":"2024-05-13T18:52:06.028802Z","shell.execute_reply":"2024-05-13T18:52:07.223355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Выше представлены изображения и маски после нормализации. Размер изображений приведен к одному.","metadata":{}},{"cell_type":"markdown","source":"# Обучение модели","metadata":{}},{"cell_type":"markdown","source":"Для обучения была выбрана модель MobileNetV2. Улучшенная версия оригинальной MobileNet и разработанная специально для задач CV. Её основной компонет - глубокая свертка, которая позволяет снижать параметры и при этом сохранять хорошее качество. Также она оперирует более широкими слоями, что улучшает качество предсказаний за счет повышенной сложности.\n\n\n- `include_top=False` задает, что верхние полносвязные слои не будут включены в загруженную модель (не будут использоваться).\n- В `names` задаются названия слоев, из которых будет состоять подграфик модели.\n- В `up_stack` определяются последовательные слои для апсемплинга. Каждый `pix2pix.upsample` слой увеличивает размер изображения в 2 раза с использованием ядра свертки размером 3x3.\n\n- Для увеличения размера изображения до начального признакового пространства используется слой `tf.keras.layers.Conv2DTranspose`.\n\n- Модель компилируется с оптимизатором Adam, функцией потерь SparseCategoricalCrossentropy, и метрикой accuracy для оценки производительности модели.\n\n","metadata":{}},{"cell_type":"code","source":"base_model = tf.keras.applications.MobileNetV2(input_shape=(128,128,3), include_top=False)\n\nnames = ['block_1_expand_relu', 'block_3_expand_relu', 'block_6_expand_relu',\n         'block_13_expand_relu', 'block_16_expand_relu']\nlayers = [base_model.get_layer(name).output for name in names]\n\ndown_sample = tf.keras.Model(base_model.input, layers)\ndown_sample.trainable = False\n\nup_stack = [\n    pix2pix.upsample(512, 3),\n    pix2pix.upsample(256, 3),  \n    pix2pix.upsample(128, 3),  \n    pix2pix.upsample(64, 3),   \n]\n\ninputs = tf.keras.layers.Input(shape=[128,128,3 ])\nx = inputs\n\nskips = down_sample(x)\nx = skips[-1]\nskips = reversed(skips[:-1])\n\nfor up, skip in zip(up_stack, skips):\n  x = up(x)\n  concat = tf.keras.layers.Concatenate()\n  x = concat([x, skip])\n\nx = tf.keras.layers.Conv2DTranspose(3, 3, strides=2 , padding='same')(x)\n\nmodel = tf.keras.Model(inputs, x)\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy']\n             )","metadata":{"execution":{"iopub.status.busy":"2024-05-13T19:14:56.767225Z","iopub.execute_input":"2024-05-13T19:14:56.767607Z","iopub.status.idle":"2024-05-13T19:14:57.639900Z","shell.execute_reply.started":"2024-05-13T19:14:56.767578Z","shell.execute_reply":"2024-05-13T19:14:57.639050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_mask(pred_mask):\n  pred_mask = tf.argmax(pred_mask, axis=-1)\n  pred_mask = pred_mask[..., tf.newaxis]\n  return pred_mask[0]\n\ndef show_predictions(dataset=None, num=1):\n    if dataset:\n        for image, mask in dataset.take(num):\n          pred_mask = model.predict(image)\n          display([image[0], mask[0], create_mask(pred_mask)])","metadata":{"execution":{"iopub.status.busy":"2024-05-13T19:11:59.319941Z","iopub.execute_input":"2024-05-13T19:11:59.320319Z","iopub.status.idle":"2024-05-13T19:11:59.327170Z","shell.execute_reply.started":"2024-05-13T19:11:59.320287Z","shell.execute_reply":"2024-05-13T19:11:59.326205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_data, \n          epochs=EPOCHS,\n          steps_per_epoch=STEPS_PER_EPOCH,\n          validation_data=test_data,\n     )","metadata":{"execution":{"iopub.status.busy":"2024-05-13T19:15:01.031880Z","iopub.execute_input":"2024-05-13T19:15:01.032570Z","iopub.status.idle":"2024-05-13T19:20:36.385561Z","shell.execute_reply.started":"2024-05-13T19:15:01.032540Z","shell.execute_reply":"2024-05-13T19:20:36.384631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"В качестве метрики для оценки качества обучения была выбрана \"accuracy\", которая характеризует качество модели, агрегированное по всем классам. Она представляет собой простую и понятную метрику. Хорошо подходит для задач классификации, где необходимо знать долю правильно угаданных классов.\n\nВ результате оценки качества модели на тестовых данных модель смогла достичь точности 90%. \nДля улучшения качества модели можно по-другому настроить значения гиперпараметров, увеличить количество эпох для обучения, также использовать другие методы аугментации для обучения модели на разнообразных данных.","metadata":{}},{"cell_type":"markdown","source":"# Предсказанные значения сегментации:","metadata":{}},{"cell_type":"code","source":"show_predictions(test_data, 6)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T19:23:38.278653Z","iopub.execute_input":"2024-05-13T19:23:38.279075Z","iopub.status.idle":"2024-05-13T19:23:44.925116Z","shell.execute_reply.started":"2024-05-13T19:23:38.279038Z","shell.execute_reply":"2024-05-13T19:23:44.924243Z"},"trusted":true},"execution_count":null,"outputs":[]}]}